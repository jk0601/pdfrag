"""
pipeline.py - ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
====================================
íŒŒì¼ ì—…ë¡œë“œë¶€í„° Supabase ì €ì¥ê¹Œì§€ì˜ ì „ì²´ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
CLI(main.py)ì™€ ì›¹(app.py) ëª¨ë‘ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì½œë°± ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

[ì²˜ë¦¬ íë¦„]
  íŒŒì¼ ì½ê¸° â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì²­í¬ ë¶„í•  â†’ ì„ë² ë”© ìƒì„± â†’ DB ì €ì¥
"""

import os
from typing import Callable

from config import Config
from processors.pdf_processor import PDFProcessor
from processors.image_processor import ImageProcessor
from processors.pptx_processor import PPTXProcessor
from rag.chunker import SemanticChunker
from rag.embedder import Embedder
from database.supabase_client import SupabaseDB

# íŒŒì¼ í™•ì¥ìë³„ ì§€ì› í˜•ì‹
SUPPORTED_EXTENSIONS = {
    ".pdf": "pdf",
    ".png": "image",
    ".jpg": "image",
    ".jpeg": "image",
    ".gif": "image",
    ".bmp": "image",
    ".tiff": "image",
    ".webp": "image",
    ".pptx": "pptx",
}


def get_file_type(file_path: str) -> str | None:
    """íŒŒì¼ í™•ì¥ìë¡œ íŒŒì¼ ì¢…ë¥˜ë¥¼ íŒë³„í•©ë‹ˆë‹¤."""
    ext = os.path.splitext(file_path)[1].lower()
    return SUPPORTED_EXTENSIONS.get(ext)


def process_file(
    file_path: str,
    ocr_enabled: bool = True,
    use_vision_api: bool = False,
) -> dict:
    """íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    file_type = get_file_type(file_path)

    if file_type is None:
        ext = os.path.splitext(file_path)[1]
        raise ValueError(
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}\n"
            f"ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_EXTENSIONS.keys())}"
        )

    if file_type == "pdf":
        processor = PDFProcessor(ocr_enabled=ocr_enabled)
    elif file_type == "image":
        processor = ImageProcessor(use_vision_api=use_vision_api)
    elif file_type == "pptx":
        processor = PPTXProcessor(ocr_enabled=ocr_enabled)
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ ìœ í˜•: {file_type}")

    return processor.process(file_path)


def ingest_file(
    file_path: str,
    ocr_enabled: bool = True,
    use_vision_api: bool = False,
    on_progress: Callable[[int, str], None] | None = None,
) -> dict:
    """
    íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  Supabaseì— ì €ì¥í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
        ocr_enabled: OCR í™œì„±í™” ì—¬ë¶€
        use_vision_api: OpenAI Vision ì‚¬ìš© ì—¬ë¶€
        on_progress: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (percent: 0~100, message: ìƒíƒœ ë©”ì‹œì§€)
                     Noneì´ë©´ ì§„í–‰ë¥ ì„ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    Returns:
        ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    file_path = os.path.abspath(file_path)

    def report(percent: int, message: str):
        if on_progress:
            on_progress(percent, message)

    # --- 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
    report(5, "ğŸ“„ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
    result = process_file(file_path, ocr_enabled, use_vision_api)
    full_text = result.get("full_text", "")

    if not full_text.strip():
        return {"error": "íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    report(
        25,
        f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ â€” {len(full_text):,}ì, "
        f"{result.get('page_count', 1)}í˜ì´ì§€",
    )

    # --- 2ë‹¨ê³„: ì²­í¬ ë¶„í•  ---
    report(30, "âœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(ì¡°ê°)ë¡œ ë¶„í• í•˜ëŠ” ì¤‘...")
    chunker = SemanticChunker()
    base_metadata = {
        "filename": result["filename"],
        "file_type": result["file_type"],
    }

    if result["file_type"] == "pdf" and "pages" in result:
        pages_data = [
            {"page_number": p.page_number, "text": p.full_text}
            for p in result["pages"]
        ]
        chunks = chunker.split_pages(pages_data, base_metadata=base_metadata)
    else:
        chunks = chunker.split_text(full_text, metadata=base_metadata)

    if not chunks:
        return {"error": "í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    report(50, f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # --- 3ë‹¨ê³„: ì„ë² ë”© ìƒì„± ---
    report(55, f"ğŸ§  {len(chunks)}ê°œ ì²­í¬ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
    embedder = Embedder()
    chunk_texts = [c.content for c in chunks]
    embeddings = embedder.embed_texts(chunk_texts)
    report(80, f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

    # --- 4ë‹¨ê³„: Supabase ì €ì¥ ---
    report(85, "ğŸ’¾ Supabase ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...")
    db = SupabaseDB()

    doc_record = db.insert_document(
        filename=result["filename"],
        file_type=result["file_type"],
        file_size=result["file_size"],
        page_count=result.get("page_count"),
    )

    chunk_data = []
    for chunk, embedding in zip(chunks, embeddings):
        chunk_data.append(
            {
                "content": chunk.content,
                "embedding": embedding,
                "metadata": chunk.metadata,
            }
        )

    db.insert_chunks(doc_record["id"], chunk_data)
    report(100, "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")

    return {
        "document_id": doc_record["id"],
        "filename": result["filename"],
        "chunk_count": len(chunks),
        "file_type": result["file_type"],
        "text_length": len(full_text),
        "page_count": result.get("page_count", 1),
    }
